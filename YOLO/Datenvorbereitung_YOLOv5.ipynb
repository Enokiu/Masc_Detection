{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übersicht über die Datensätze\n",
    "\n",
    "In dieser Arbeit wurden zwei Kaggle-Datensätze zur Maskenerkennung verwendet. Der erste Datensatz namens \"Face Mask Detection Dataset\", steht unter [diesem Link](https://www.kaggle.com/datasets/wobotintelligence/face-mask-detection-dataset) zur Verfügung und enthält JSON-Annotationen. Der zweite Datensatz, \"Kaggle Face Mask Detection Full\", kann unter [diesem Link](https://www.kaggle.com/datasets/andrewmvd/face-mask-detection) gefunden werden. Dieser Datensatz zeichnet sich durch XML-Annotationen aus und wurde durch gezielte Augmentation erweitert, um Bilder mit schwierigen Lichtverhältnissen zu simulieren und einen Datensatz für nächtliche Bedingungen zu generieren.\n",
    "\n",
    "## Klassen im nicht-dunklen Datensatz (20 Klassen):\n",
    "| Klassenindex | Klasse                  |\n",
    "|--------------|-------------------------|\n",
    "| 0            | face_no_mask            |\n",
    "| 1            | face_with_mask          |\n",
    "| 2            | mask_surgical           |\n",
    "| 3            | hat                     |\n",
    "| 4            | eyeglasses              |\n",
    "| 5            | face_other_covering     |\n",
    "| 6            | face_with_mask_incorrect|\n",
    "| 7            | mask_colorful           |\n",
    "| 8            | helmet                  |\n",
    "| 9            | sunglasses              |\n",
    "| 10           | scarf_bandana           |\n",
    "| 11           | hair_net                |\n",
    "| 12           | goggles                 |\n",
    "| 13           | face_shield             |\n",
    "| 14           | hijab_niqab             |\n",
    "| 15           | turban                  |\n",
    "| 16           | balaclava_ski_mask      |\n",
    "| 17           | gas_mask                |\n",
    "| 18           | hood                    |\n",
    "| 19           | other                   |\n",
    "\n",
    "## Klassen im dunklen Datensatz (3 Klassen):\n",
    "| Klassenindex | Klasse                  |\n",
    "|--------------|-------------------------|\n",
    "| 0            | without_mask            |\n",
    "| 1            | with_mask               |\n",
    "| 2            | mask_weared_incorrect   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Struktur der zu generierenden Datensätze für YOLO-Training\n",
    "\n",
    "Die Datenvorbereitung umfasst die Konvertierung und Aufbereitung der Datensätze für das YOLO-Training. Die generierten Datensätze werden im Verzeichnis \"YOLO/dataset\" strukturiert abgelegt und umfassen folgende Untergruppen:\n",
    "\n",
    "1. **all_classes:** Enthält alle Klassen und wird beim Training auf dem nicht-dunklen Datensatz verwendet.\n",
    "2. **face_classes:** Berücksichtigt nur die Gesichtsklassen von den insgesamt 20 Klassen während des Trainings auf dem nicht-dunklen Datensatz.\n",
    "3. **single_person:** Bezieht sich ausschließlich auf die Gesichtsklassen und einzelne Personen beim Training auf dem nicht-dunklen Datensatz.\n",
    "4. **dark_dataset:** Umfasst alle Klassen und wird beim Training auf dem dunklen Datensatz verwendet.\n",
    "\n",
    "\n",
    "# Konvertierung ins YOLO-Format\n",
    "Für das YOLO-Training müssen Annotationen ins YOLO-Format (txt) konvertiert werden. Die Konvertierung von JSON-Daten folgt diesen Regeln:\n",
    "\n",
    "- Erste Spalte: Klassenindizes\n",
    "- Zweite bis fünfte Spalte: Koordinaten der Bounding Box (x, y, w, h), diese müssen normalisiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import cv2\n",
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erstellung des All_Classes-Datensatzes für das Training\n",
    "\n",
    "In diesem Abschnitt werden alle 20 Klassen für das Training berücksichtigt. Der Datensatz wird für das YOLO-Training in 80% Trainings- und 20% Validierungsdatensatz aufgeteilt. Die Annotationen im Datensatz sind im JSON-Format gespeichert und müssen für das YOLO-Training noch ins YOLO-Format umgewandelt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Überprüfung, ob eine Datei Face-Klassen enthält\n",
    "def contains_face_classes(json_path, class_mapping):\n",
    "    with open(json_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    for bbox in json_data[\"Annotations\"]:\n",
    "        class_name = bbox[\"classname\"]\n",
    "        if class_name in class_mapping:\n",
    "            return True\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Konvertierung der Labels ins YOLOv5-Format\n",
    "def convert_labels_to_yolov5_format(json_path, image_path, output_dir, class_mapping):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Erstelle YOLOv5-Annotationen\n",
    "    with open(json_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    image_height, image_width, _ = image.shape\n",
    "\n",
    "    yolo_annotations = []\n",
    "    for bbox in json_data[\"Annotations\"]:\n",
    "        x_min, y_min, x_max, y_max = bbox[\"BoundingBox\"]\n",
    "        class_name = bbox[\"classname\"]\n",
    "\n",
    "        if class_name not in class_mapping:\n",
    "            continue  # Überspringe nicht relevante Klassen\n",
    "\n",
    "        # Klassenindizierung\n",
    "        class_index = class_mapping[class_name]\n",
    "\n",
    "        # Normalisierung der Koordinaten\n",
    "        x_center = (x_min + x_max) / (2 * image_width)\n",
    "        y_center = (y_min + y_max) / (2 * image_height)\n",
    "        width = (x_max - x_min) / image_width\n",
    "        height = (y_max - y_min) / image_height\n",
    "\n",
    "        yolo_annotation = f\"{class_index} {x_center} {y_center} {width} {height}\"\n",
    "        yolo_annotations.append(yolo_annotation)\n",
    "\n",
    "    image_filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    output_filename = f\"{image_filename}.txt\"\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "    # Erstelle YOLOv5-Annotationen\n",
    "    with open(output_path, 'w') as f:\n",
    "        for annotation in yolo_annotations:\n",
    "            f.write(annotation + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfade zu den Daten\n",
    "json_dir = \"../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/annotations\"\n",
    "image_dir = \"../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/images\"\n",
    "dataset_output_dir = \"dataset/all_classes\"\n",
    "\n",
    "# Klassen Mapping\n",
    "class_mapping_all = {\n",
    "    \"face_no_mask\": 0,\n",
    "    \"face_with_mask\": 1,\n",
    "    \"mask_surgical\": 2,\n",
    "    \"hat\": 3,\n",
    "    \"eyeglasses\": 4,\n",
    "    \"face_other_covering\": 5,\n",
    "    \"face_with_mask_incorrect\": 6,\n",
    "    \"mask_colorful\": 7,\n",
    "    \"helmet\": 8,\n",
    "    \"sunglasses\": 9,\n",
    "    \"scarf_bandana\": 10,\n",
    "    \"hair_net\": 11,\n",
    "    \"goggles\": 12,\n",
    "    \"face_shield\": 13,\n",
    "    \"hijab_niqab\": 14,\n",
    "    \"turban\": 15,\n",
    "    \"balaclava_ski_mask\": 16,\n",
    "    \"gas_mask\": 17,\n",
    "    \"hood\": 18,\n",
    "    \"other\": 19\n",
    "}\n",
    "\n",
    "\n",
    "# Bilder und Annotationen einlesen\n",
    "images = [os.path.join(image_dir, x) for x in os.listdir(image_dir)]\n",
    "annotations = [os.path.join(json_dir, x) for x in os.listdir(json_dir) if x.endswith(\".json\")]\n",
    "\n",
    "# Train-Test-Split\n",
    "if len(images) < 2 or len(annotations) < 2:\n",
    "    print(\"Nicht genügend Datenpunkte für den Split vorhanden.\")\n",
    "    exit()\n",
    "\n",
    "train_images, test_images, train_annotations, test_annotations = train_test_split(images, annotations, test_size=0.3, random_state=1)\n",
    "val_images, test_images, val_annotations, test_annotations = train_test_split(test_images, test_annotations, test_size=0.5, random_state=1)\n",
    "\n",
    "# Anwendung auf Trainingsdaten\n",
    "for image_path, json_path in zip(train_images, train_annotations):\n",
    "    convert_labels_to_yolov5_format(json_path, image_path, os.path.join(dataset_output_dir, 'labels', 'train'), class_mapping_all)\n",
    "\n",
    "# Anwendung auf Validierungsdaten\n",
    "for image_path, json_path in zip(val_images, val_annotations):\n",
    "    convert_labels_to_yolov5_format(json_path, image_path, os.path.join(dataset_output_dir, 'labels', 'val'), class_mapping_all)\n",
    "\n",
    "# Kopiere Bilder in die entsprechenden Ordner\n",
    "for split, split_images in zip(['train', 'val'], [train_images, val_images]):\n",
    "    split_images_output_dir = os.path.join(dataset_output_dir, 'images', split)\n",
    "    os.makedirs(split_images_output_dir, exist_ok=True)\n",
    "    for image_path in split_images:\n",
    "        image_filename = os.path.basename(image_path)\n",
    "        shutil.copy(image_path, os.path.join(split_images_output_dir, image_filename))\n",
    "\n",
    "# Erstelle classes.txt-Datei, mit allen Klassen und deren Indizierung\n",
    "for split in ['train', 'val']:\n",
    "    classes_file_path = os.path.join(dataset_output_dir, 'labels', split, 'classes.txt')\n",
    "    with open(classes_file_path, 'w') as f:\n",
    "        for class_name, class_index in class_mapping_all.items():\n",
    "            f.write(f\"{class_index} {class_name}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erstellung des Face_Classes-Datensatzes für das Training\n",
    "\n",
    "In diesem Abschnitt werden nur die 4 Face-Klassen (siehe unten) aus dem nicht-dunklen Datensatz betrachtet. Hierfür werden nicht relevante Bilder und Labels im Datensatz identifiziert und gelöscht. Anschließend erfolgt die Aufteilung der Daten in 80% Training und 20% Validierungsdatensatz.\n",
    "\n",
    "| Klassenindex | Klasse                  |\n",
    "|--------------|-------------------------|\n",
    "| 0            | face_no_mask            |\n",
    "| 1            | face_with_mask          |\n",
    "| 2            | face_other_covering     |\n",
    "| 3            | face_with_mask_incorrect|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Entfernung leerer Dateien\n",
    "def remove_empty_files(json_dir, images_dir, class_mapping):\n",
    "    for json_filename in os.listdir(json_dir):\n",
    "        if json_filename.endswith(\".json\"):\n",
    "            json_path = os.path.join(json_dir, json_filename)\n",
    "            image_name, _ = os.path.splitext(json_filename)  # Extract image name without the \".json\" extension\n",
    "\n",
    "            # Construct the image path\n",
    "            image_path = os.path.join(images_dir, image_name)\n",
    "\n",
    "            # Check if the .json file is empty or does not contain face classes\n",
    "            if os.path.getsize(json_path) == 0 or not contains_face_classes(json_path, class_mapping):\n",
    "                print(f\"Delete Labeldata: {json_path}\")\n",
    "                os.remove(json_path)\n",
    "\n",
    "                # Delete the corresponding picture if available\n",
    "                if os.path.exists(image_path):\n",
    "                    print(f\"Delete the corresponding picture: {image_path}\")\n",
    "                    os.remove(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete Labeldata: ../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/annotations\\1855.jpg.json\n",
      "Delete the corresponding picture: ../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/images\\1855.jpg\n",
      "Delete Labeldata: ../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/annotations\\1945.jpg.json\n",
      "Delete the corresponding picture: ../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/images\\1945.jpg\n",
      "Delete Labeldata: ../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/annotations\\2135.jpg.json\n",
      "Delete the corresponding picture: ../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/images\\2135.jpg\n",
      "Delete Labeldata: ../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/annotations\\3102.png.json\n",
      "Delete the corresponding picture: ../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/images\\3102.png\n",
      "Delete Labeldata: ../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/annotations\\4618.png.json\n",
      "Delete the corresponding picture: ../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/images\\4618.png\n",
      "Delete Labeldata: ../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/annotations\\5012.jpg.json\n",
      "Delete the corresponding picture: ../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/images\\5012.jpg\n",
      "Delete Labeldata: ../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/annotations\\5023.jpg.json\n",
      "Delete the corresponding picture: ../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/images\\5023.jpg\n",
      "Delete Labeldata: ../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/annotations\\5052.jpg.json\n",
      "Delete the corresponding picture: ../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/images\\5052.jpg\n",
      "Delete Labeldata: ../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/annotations\\5277.jpg.json\n",
      "Delete the corresponding picture: ../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/images\\5277.jpg\n",
      "Delete Labeldata: ../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/annotations\\5418.jpg.json\n",
      "Delete the corresponding picture: ../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/images\\5418.jpg\n",
      "Delete Labeldata: ../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/annotations\\5430.jpg.json\n",
      "Delete the corresponding picture: ../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/images\\5430.jpg\n",
      "Delete Labeldata: ../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/annotations\\5443.jpg.json\n",
      "Delete the corresponding picture: ../Datasets/Face Mask Detection Dataset/Medical mask/Medical mask/Medical Mask/images\\5443.jpg\n"
     ]
    }
   ],
   "source": [
    "# Pfade zu den Daten\n",
    "dataset_output_dir = \"dataset/face_classes\"\n",
    "\n",
    "# Klassen Mapping\n",
    "relevant_classes = [\"face_no_mask\", \"face_with_mask\", \"face_other_covering\", \"face_with_mask_incorrect\"]\n",
    "class_mapping = {class_name: idx for idx, class_name in enumerate(relevant_classes)}\n",
    "\n",
    "# Entferne leere und nicht relevante Dateien\n",
    "remove_empty_files(json_dir, image_dir, class_mapping)\n",
    "\n",
    "# Bilder und Annotationen einlesen\n",
    "images = [os.path.join(image_dir, x) for x in os.listdir(image_dir)]\n",
    "annotations = [os.path.join(json_dir, x) for x in os.listdir(json_dir) if x.endswith(\".json\")]\n",
    "\n",
    "# Train-Test-Split\n",
    "if len(images) < 2 or len(annotations) < 2:\n",
    "    print(\"Nicht genügend Datenpunkte für den Split vorhanden.\")\n",
    "    exit()\n",
    "\n",
    "train_images, test_images, train_annotations, test_annotations = train_test_split(images, annotations, test_size=0.3, random_state=1)\n",
    "val_images, test_images, val_annotations, test_annotations = train_test_split(test_images, test_annotations, test_size=0.5, random_state=1)\n",
    "\n",
    "# Anwendung auf Trainingsdaten\n",
    "for image_path, json_path in zip(train_images, train_annotations):\n",
    "    convert_labels_to_yolov5_format(json_path, image_path, os.path.join(dataset_output_dir, 'labels', 'train'), class_mapping)\n",
    "\n",
    "# Anwendung auf Validierungsdaten\n",
    "for image_path, json_path in zip(val_images, val_annotations):\n",
    "    convert_labels_to_yolov5_format(json_path, image_path, os.path.join(dataset_output_dir, 'labels', 'val'), class_mapping)\n",
    "\n",
    "# Kopiere Bilder in die entsprechenden Ordner\n",
    "for split, split_images in zip(['train', 'val'], [train_images, val_images]):\n",
    "    split_images_output_dir = os.path.join(dataset_output_dir, 'images', split)\n",
    "    os.makedirs(split_images_output_dir, exist_ok=True)\n",
    "    for image_path in split_images:\n",
    "        image_filename = os.path.basename(image_path)\n",
    "        shutil.copy(image_path, os.path.join(split_images_output_dir, image_filename))\n",
    "\n",
    "# Erstelle classes.txt-Datei, mit allen Klassen und deren Indizierung\n",
    "for split in ['train', 'val']:\n",
    "    classes_file_path = os.path.join(dataset_output_dir, 'labels', split, 'classes.txt')\n",
    "    with open(classes_file_path, 'w') as f:\n",
    "        for class_name, class_index in class_mapping.items():\n",
    "            f.write(f\"{class_index} {class_name}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einzel-Personen-Datensatz\n",
    "\n",
    "In diesem Datensatz wird ebenfalls der nicht-dunkle Datensatz mit den JSON-Annotationen verarbeitet. Der Fokus liegt hier ausschließlich auf einzelnen Gesichtern. Dabei werden nur die 4 Face_Classes berücksichtigt. Es ist bekannt, dass in einer TXT-Datei mehrere Face_Classes darauf hinweisen, dass mehrere Personen erkannt wurden. Solche TXT-Dateien und die dazugehörigen Bilder werden dann in den Ordner \"single_person\" kopiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtert die Einzelpersonen vor dem Split\n",
    "def filter_single_persons(labels_dir, images_dir, output_labels_dir, output_images_dir):\n",
    "    if not os.path.exists(output_labels_dir):\n",
    "        os.makedirs(output_labels_dir)\n",
    "    if not os.path.exists(output_images_dir):\n",
    "        os.makedirs(output_images_dir)\n",
    "\n",
    "    for split in ['train', 'val']:\n",
    "        split_labels_dir = os.path.join(labels_dir, split)\n",
    "        split_images_dir = os.path.join(images_dir, split)\n",
    "\n",
    "        for filename in os.listdir(split_labels_dir):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                txt_path = os.path.join(split_labels_dir, filename)\n",
    "                image_name, _ = os.path.splitext(filename)\n",
    "\n",
    "                # Lese die Anzahl der Personen aus der .txt-Datei\n",
    "                with open(txt_path, 'r') as f:\n",
    "                    num_persons = sum(1 for line in f)\n",
    "\n",
    "                # Wenn nur eine Person vorhanden ist, kopiere die .txt-Datei und das Bild\n",
    "                if num_persons == 1:\n",
    "                    output_txt_path = os.path.join(output_labels_dir, filename)\n",
    "\n",
    "                    # Suche nach dem Bild mit verschiedenen Erweiterungen (.jpg, .jpeg, .png)\n",
    "                    image_extensions = [\".jpg\", \".jpeg\", \".png\"]\n",
    "                    for ext in image_extensions:\n",
    "                        image_path = os.path.join(split_images_dir, f\"{image_name}{ext}\")\n",
    "                        if os.path.exists(image_path):\n",
    "                            break\n",
    "\n",
    "                    if os.path.exists(image_path):\n",
    "                        output_image_path = os.path.join(output_images_dir, f\"{image_name}{ext}\")\n",
    "                        shutil.copy(txt_path, output_txt_path)\n",
    "                        shutil.copy(image_path, output_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset/single_person/labels\\\\val\\\\classes.txt'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pfade zu den Verzeichnissen\n",
    "labels_output_dir = \"dataset/face_classes/labels\"\n",
    "image_output_dir = \"dataset/face_classes/images\"\n",
    "single_dataset_output_labels_dir = \"dataset/single_person/labels\"\n",
    "single_dataset_output_images_dir = \"dataset/single_person/images\"\n",
    "\n",
    "# Filtere Einzelpersonen vor dem Split\n",
    "filter_single_persons(labels_output_dir, image_output_dir, single_dataset_output_labels_dir, single_dataset_output_images_dir)\n",
    "\n",
    "# Train-Test-Split für die Einzelpersonen Dataset\n",
    "single_dataset_images = [os.path.join(single_dataset_output_images_dir, x) for x in os.listdir(single_dataset_output_images_dir)]\n",
    "single_dataset_labels = [os.path.join(single_dataset_output_labels_dir, x) for x in os.listdir(single_dataset_output_labels_dir)]\n",
    "\n",
    "if len(single_dataset_images) < 2 or len(single_dataset_labels) < 2:\n",
    "    print(\"Nicht genügend Datenpunkte für den Split vorhanden.\")\n",
    "    exit()\n",
    "\n",
    "single_train_images, single_val_images, single_train_labels, single_val_labels = train_test_split(\n",
    "    single_dataset_images, single_dataset_labels, test_size=0.3, random_state=1\n",
    ")\n",
    "\n",
    "# Speichern Sie die gesplitteten Daten in den entsprechenden Ordnern\n",
    "for split, images, labels in zip(['train', 'val'],\n",
    "                                [single_train_images, single_val_images],\n",
    "                                [single_train_labels, single_val_labels]):\n",
    "    split_images_output_dir = os.path.join(single_dataset_output_images_dir, split)\n",
    "    os.makedirs(split_images_output_dir, exist_ok=True)\n",
    "    for image_path in images:\n",
    "        image_filename = os.path.basename(image_path)\n",
    "        shutil.move(image_path, os.path.join(split_images_output_dir, image_filename))\n",
    "\n",
    "    split_labels_output_dir = os.path.join(single_dataset_output_labels_dir, split)\n",
    "    os.makedirs(split_labels_output_dir, exist_ok=True)\n",
    "    for label_path in labels:\n",
    "        label_filename = os.path.basename(label_path)\n",
    "        shutil.move(label_path, os.path.join(split_labels_output_dir, label_filename))\n",
    "\n",
    "\n",
    "# Pfad zu deiner classes.txt Datei\n",
    "classes_txt_path = os.path.join(labels_output_dir, 'train', 'classes.txt')\n",
    "\n",
    "# Kopiere die classes.txt Datei\n",
    "shutil.copy(classes_txt_path, os.path.join(single_dataset_output_labels_dir, 'train', 'classes.txt'))\n",
    "shutil.copy(classes_txt_path, os.path.join(single_dataset_output_labels_dir, 'val', 'classes.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dunkler Datensatz\n",
    "\n",
    "Der dunkle Datensatz wurde durch Augmentation verdunkelt, da in Kaggle keine dunklen Datensätze zu finden sind. Das Ziel besteht darin, ein Modell zu trainieren, das auch im Dunkeln Masken erkennen kann. In diesem Datensatz werden alle Klassen berücksichtigt, da es 3 relevante Klassen gibt. Es ist wichtig zu beachten, dass die Annotationen für diesen Datensatz als XML-Dateien vorliegen und für das YOLO-Training in das YOLO-Format konvertiert werden müssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Konvertierung der XML-Datei ins YOLOv5-Format\n",
    "def convert_xml_to_yolo(xml_path, image_directory, output_dir, class_mapping):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Öffne XML-Datei\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Holen Sie sich den Dateinamen des Bildes (ohne Erweiterung)\n",
    "    image_filename_without_extension = os.path.splitext(root.find('filename').text)[0]\n",
    "\n",
    "    # Bildpfad\n",
    "    image_path = os.path.join(image_directory, f\"{image_filename_without_extension}.png\")\n",
    "\n",
    "    # Überprüfen Sie, ob das Bild vorhanden ist\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Image file not found: {image_path}\")\n",
    "        return\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    image_height, image_width, _ = image.shape\n",
    "\n",
    "    yolo_annotations = []\n",
    "    for obj in root.findall('object'):\n",
    "        class_name = obj.find('name').text\n",
    "\n",
    "        if class_name not in class_mapping:\n",
    "            raise ValueError(f\"Ungültige Klasse: {class_name}\")\n",
    "\n",
    "        # Klassenindizierung\n",
    "        class_index = class_mapping[class_name]\n",
    "\n",
    "        # Bounding Box Koordinaten\n",
    "        bbox = obj.find('bndbox')\n",
    "        x_min = float(bbox.find('xmin').text)\n",
    "        y_min = float(bbox.find('ymin').text)\n",
    "        x_max = float(bbox.find('xmax').text)\n",
    "        y_max = float(bbox.find('ymax').text)\n",
    "\n",
    "        # Normalisierung der Koordinaten\n",
    "        x_center = (x_min + x_max) / (2 * image_width)\n",
    "        y_center = (y_min + y_max) / (2 * image_height)\n",
    "        width = (x_max - x_min) / image_width\n",
    "        height = (y_max - y_min) / image_height\n",
    "\n",
    "        yolo_annotation = f\"{class_index} {x_center} {y_center} {width} {height}\"\n",
    "        yolo_annotations.append(yolo_annotation)\n",
    "\n",
    "    output_filename = f\"{image_filename_without_extension}.txt\"\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "    with open(output_path, 'w') as f:\n",
    "        for annotation in yolo_annotations:\n",
    "            f.write(annotation + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfade zu den Daten\n",
    "xml_directory = \"../Datasets/Kaggle Face Mask Detection Full/annotations\"\n",
    "image_directory = \"../Datasets/Kaggle Face Mask Detection Full/dark/images\"\n",
    "dark_labels_output_dir = \"dataset/dark_dataset/labels\"\n",
    "dark_images_output_dir = \"dataset/dark_dataset/images\"\n",
    "\n",
    "# Klassenmapping\n",
    "class_mapping_dark = {\n",
    "    \"without_mask\": 0,\n",
    "    \"with_mask\": 1,\n",
    "    \"mask_weared_incorrect\": 2,\n",
    "}\n",
    "\n",
    "# Erstelle Dark Dataset-Ordner, wenn nicht vorhanden\n",
    "if not os.path.exists(dark_labels_output_dir):\n",
    "    os.makedirs(dark_labels_output_dir)\n",
    "if not os.path.exists(dark_images_output_dir):\n",
    "    os.makedirs(dark_images_output_dir)\n",
    "\n",
    "# Liste für Bilder und Annotationen\n",
    "dark_images = []\n",
    "dark_labels = []\n",
    "\n",
    "# Durchlaufe die XML-Dateien\n",
    "for xml_filename in os.listdir(xml_directory):\n",
    "    if xml_filename.endswith(\".xml\"):\n",
    "        xml_path = os.path.join(xml_directory, xml_filename)\n",
    "\n",
    "        # Konvertiere XML zu YOLOv5-Format\n",
    "        convert_xml_to_yolo(xml_path, image_directory, dark_labels_output_dir, class_mapping_dark)\n",
    "\n",
    "        # Extrahiere Bildname ohne Erweiterung\n",
    "        image_name = os.path.splitext(xml_filename)[0]\n",
    "\n",
    "        # Füge Bildpfad zur Liste hinzu\n",
    "        image_path = os.path.join(image_directory, f\"{image_name}.png\")\n",
    "        dark_images.append(image_path)\n",
    "\n",
    "        # Füge Labelpfad zur Liste hinzu\n",
    "        label_name = f\"{image_name}.txt\"\n",
    "        label_path = os.path.join(dark_labels_output_dir, label_name)\n",
    "        dark_labels.append(label_path)\n",
    "\n",
    "# Train-Test-Split\n",
    "if len(dark_images) < 2:\n",
    "    print(\"Nicht genügend Datenpunkte für den Split vorhanden.\")\n",
    "    exit()\n",
    "\n",
    "dark_train_images, dark_val_images, dark_train_labels, dark_val_labels = train_test_split(\n",
    "    dark_images, dark_labels, test_size=0.3, random_state=1\n",
    ")\n",
    "\n",
    "# Kopiere Bilder und Labels in den Dark Dataset-Ordner für Training\n",
    "for image_path, label_path in zip(dark_train_images, dark_train_labels):\n",
    "    image_name = os.path.basename(image_path)\n",
    "    label_name = os.path.basename(label_path)\n",
    "\n",
    "    output_image_dir = os.path.join(dark_images_output_dir, \"train\")\n",
    "    output_label_dir = os.path.join(dark_labels_output_dir, \"train\")\n",
    "\n",
    "    # Überprüfe, ob die Zielordner existieren, und erstelle sie, falls nicht\n",
    "    if not os.path.exists(output_image_dir):\n",
    "        os.makedirs(output_image_dir)\n",
    "    if not os.path.exists(output_label_dir):\n",
    "        os.makedirs(output_label_dir)\n",
    "\n",
    "    output_image_path = os.path.join(output_image_dir, image_name)\n",
    "    output_label_path = os.path.join(output_label_dir, label_name)\n",
    "\n",
    "    shutil.copy(image_path, output_image_path)\n",
    "    shutil.copy(label_path, output_label_path)\n",
    "\n",
    "# Kopiere Bilder und Labels in den Dark Dataset-Ordner für Validierung\n",
    "for image_path, label_path in zip(dark_val_images, dark_val_labels):\n",
    "    image_name = os.path.basename(image_path)\n",
    "    label_name = os.path.basename(label_path)\n",
    "\n",
    "    output_image_dir = os.path.join(dark_images_output_dir, \"val\")\n",
    "    output_label_dir = os.path.join(dark_labels_output_dir, \"val\")\n",
    "\n",
    "    # Überprüfe, ob die Zielordner existieren, und erstelle sie, falls nicht\n",
    "    if not os.path.exists(output_image_dir):\n",
    "        os.makedirs(output_image_dir)\n",
    "    if not os.path.exists(output_label_dir):\n",
    "        os.makedirs(output_label_dir)\n",
    "\n",
    "    output_image_path = os.path.join(output_image_dir, image_name)\n",
    "    output_label_path = os.path.join(output_label_dir, label_name)\n",
    "\n",
    "    shutil.copy(image_path, output_image_path)\n",
    "    shutil.copy(label_path, output_label_path)\n",
    "\n",
    "# Erstelle classes.txt-Datei, mit allen Klassen und deren Indizierung\n",
    "classes_file_path = os.path.join(dark_labels_output_dir, \"classes.txt\")\n",
    "with open(classes_file_path, 'w') as f:\n",
    "    for class_name, class_index in class_mapping_dark.items():\n",
    "        f.write(f\"{class_index} {class_name}\\n\")\n",
    "\n",
    "# Kopiere classes.txt in den Trainingsordner\n",
    "output_train_classes_path = os.path.join(dark_labels_output_dir, \"train\", \"classes.txt\")\n",
    "shutil.copy(classes_file_path, output_train_classes_path)\n",
    "\n",
    "# Kopiere classes.txt in den Validierungsordner\n",
    "output_val_classes_path = os.path.join(dark_labels_output_dir, \"val\", \"classes.txt\")\n",
    "shutil.copy(classes_file_path, output_val_classes_path)\n",
    "\n",
    "# Lösche alle .txt-Dateien im labels-Ordner\n",
    "file_extensions_to_delete = [\".txt\"]\n",
    "\n",
    "# Lösche alle .txt-Dateien im labels-Ordner\n",
    "for filename in os.listdir(dark_labels_output_dir):\n",
    "    if filename.endswith(tuple(file_extensions_to_delete)):\n",
    "        file_path = os.path.join(dark_labels_output_dir, filename)\n",
    "        os.remove(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YAML-Datei für alle Datensätze erstellen\n",
    "Die YAML-Datei ist eine Konfigurationsdatei für das Training des YOLO-Modells. Diese ist wie folgt aufgebaut:\n",
    "- `train`: Pfad zum Verzeichnis, das die Trainingsbilder enthält.\n",
    "- `val`: Pfad zum Verzeichnis mit den Validierungsbildern.\n",
    "- `nc`: Anzahl der Klassen im Datensatz.\n",
    "- `names`: Eine Liste der Klassennamen. \n",
    "- `patience`: Anzahl der Epochen, die gewartet werden, wenn die Leistung des Modells nicht besser wird, bevor das Training beendet wird. Hierbei kann bei Overfitting das Training automatisch abgebrochen werden.\n",
    "- `delta`: Schwellenwert, um zu bestimmen, ob die Leistung des Modells sich verbessert hat. Wenn die Verbesserung kleiner als dieser Schwellenwert ist, wird das Training gestoppt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_yaml_file(data_path, train_path, val_path, class_names_file):\n",
    "    with open(class_names_file, 'r') as f:\n",
    "        class_names = [line.strip().split(' ', 1)[1] for line in f.readlines()]\n",
    "\n",
    "    data_yaml = f\"train: {train_path}\\n\" \\\n",
    "                f\"val: {val_path}\\n\" \\\n",
    "                f\"nc: {len(class_names)}\\n\" \\\n",
    "                f\"names: {class_names}\\n\" \\\n",
    "                f\"patience: 5  # Anzahl der Epochen, die gewartet werden, wenn die Leistung nicht besser wird\\n\" \\\n",
    "                f\"delta: 0.0001  # Schwellenwert, um zu bestimmen, ob die Leistung sich verbessert hat\\n\" \n",
    "    \n",
    "    with open(data_path, 'w') as file:\n",
    "        file.write(data_yaml)\n",
    "    \n",
    "    print(f\"Die data.yaml-Datei wurde erfolgreich erstellt: {data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die data.yaml-Datei wurde erfolgreich erstellt: dataset/all_classes/data.yaml\n"
     ]
    }
   ],
   "source": [
    "#YAML-Datei für all_classes\n",
    "create_data_yaml_file(  \"dataset/all_classes/data.yaml\",\n",
    "                        \"../dataset/all_classes/images/train/\",\n",
    "                        \"../dataset/all_classes/images/val/\",\n",
    "                        \"dataset/all_classes/labels/train/classes.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die data.yaml-Datei wurde erfolgreich erstellt: dataset/face_classes/data.yaml\n"
     ]
    }
   ],
   "source": [
    "#YAML-Datei für face_classes\n",
    "create_data_yaml_file(  \"dataset/face_classes/data.yaml\",\n",
    "                        \"../dataset/face_classes/images/train/\",\n",
    "                        \"../dataset/face_classes/images/val/\",\n",
    "                        \"dataset/face_classes/labels/train/classes.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die data.yaml-Datei wurde erfolgreich erstellt: dataset/single_person/data.yaml\n"
     ]
    }
   ],
   "source": [
    "#YAML-Datei für single_person\n",
    "create_data_yaml_file(  \"dataset/single_person/data.yaml\",\n",
    "                        \"../dataset/single_person/images/train/\",\n",
    "                        \"../dataset/single_person/images/val/\",\n",
    "                        \"dataset/single_person/labels/train/classes.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die data.yaml-Datei wurde erfolgreich erstellt: dataset/dark_dataset/data.yaml\n"
     ]
    }
   ],
   "source": [
    "#YAML-Datei für dark_dataset\n",
    "create_data_yaml_file(  \"dataset/dark_dataset/data.yaml\",\n",
    "                        \"../dataset/dark_dataset/images/train/\",\n",
    "                        \"../dataset/dark_dataset/images/val/\",\n",
    "                        \"dataset/dark_dataset/labels/train/classes.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
